{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-google-genai streamlit sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Embeddings using sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (384,)\n",
      "Embeddings:  [-0.00285213 -0.08175396  0.08026796 -0.00358492 -0.03835627 -0.05097928\n",
      " -0.06526164 -0.0693114  -0.02728982  0.04648076 -0.02619143 -0.0344762\n",
      "  0.03024899  0.00664412 -0.05433209 -0.03604044 -0.04957154  0.04955317\n",
      " -0.04182218 -0.08995242 -0.02662038 -0.0399776  -0.01725702 -0.03215188\n",
      "  0.04945602]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "sentences = [\n",
    "    \"Machine learning is fun!\",\n",
    "    \"I love artificial intelligence.\",\n",
    "    \"The sky is blue today.\",\n",
    "    \"AI and ML are closely related fields.\"\n",
    "]\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "print(\"Embedding shape:\", embeddings[0].shape)\n",
    "print(\"Embeddings: \", embeddings[0][:25]) # There are total upto 384, but lets just print first 26 values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a list of some **popular and commonly used models** from the `sentence-transformers` library, along with their **embedding sizes**, **architecture**, and **key use cases**:\n",
    "\n",
    "\n",
    "| Model Name                                | Embedding Size | Architecture       | Notes / Use Case                                               |\n",
    "| ----------------------------------------- | -------------- | ------------------ | -------------------------------------------------------------- |\n",
    "| **all-MiniLM-L6-v2**                      | **384**        | MiniLM (6 layers)  | Very fast, general-purpose                                     |\n",
    "| **all-MiniLM-L12-v2**                     | **384**        | MiniLM (12 layers) | Better accuracy than L6                                        |\n",
    "| **all-mpnet-base-v2**                     | **768**        | MPNet-base         | Very accurate general-purpose                                  |\n",
    "| **paraphrase-MiniLM-L6-v2**               | **384**        | MiniLM (6 layers)  | Trained specifically for paraphrase similarity                 |\n",
    "| **paraphrase-multilingual-MiniLM-L12-v2** | **768**        | MiniLM (12 layers) | Multilingual, supports \\~50+ languages                         |\n",
    "| **multi-qa-MiniLM-L6-cos-v1**             | **384**        | MiniLM (6 layers)  | Trained for QA retrieval tasks                                 |\n",
    "| **multi-qa-mpnet-base-dot-v1**            | **768**        | MPNet-base         | Best for multi-lingual semantic search                         |\n",
    "| **distiluse-base-multilingual-cased-v2**  | **512**        | DistilBERT         | Multilingual + general-purpose                                 |\n",
    "| **gtr-t5-base**                           | **768**        | T5 encoder (base)  | State-of-the-art for semantic similarity, supports longer text |\n",
    "| **gtr-t5-large**                          | **1024**       | T5 encoder (large) | Higher accuracy, slower and memory-heavy                       |\n",
    "| **nli-roberta-base-v2**                   | **768**        | RoBERTa-base       | Trained on NLI data for sentence similarity                    |\n",
    "| **sentence-t5-base**                      | **768**        | T5-base            | Optimized for semantic embeddings via ST5 framework            |\n",
    "\n",
    "---\n",
    "\n",
    "### Notes:\n",
    "\n",
    "* **MiniLM models (384-dim)**: Great balance of speed and performance for real-time tasks.\n",
    "* **MPNet and RoBERTa (768-dim)**: Larger, slower, but more accurate embeddings.\n",
    "* **Multilingual models**: Like `paraphrase-multilingual-MiniLM-L12-v2` or `distiluse-...`, support cross-lingual applications.\n",
    "* **T5 / GTR models**: Encoder-decoder transformers — great for long text and QA-style semantic tasks, but more resource intensive.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
