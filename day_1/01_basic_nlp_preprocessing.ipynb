{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a53c4d8",
   "metadata": {},
   "source": [
    "# Basic NLP Text Preprocessing\n",
    "\n",
    "## Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be6906f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!!! This is GREAT for learning NLP preprocessing... I'm really helpfull!\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello World!!! This is GREAT for learning NLP preprocessing... I'm really helpfull!\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c559a1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello World!!! This is GREAT for learning NLP preprocessing... I'm really helpfull!\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e32936c",
   "metadata": {},
   "source": [
    "## 1. Basic Python String Operations\n",
    "\n",
    "### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e6228eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hello world!!! this is great for learning nlp preprocessing... i'm really helpfull!\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowered = text.lower()\n",
    "lowered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b090e077",
   "metadata": {},
   "source": [
    "### Strip Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63940ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello  world'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messy_text = \"  hello  world  \"\n",
    "cleaned = messy_text.strip()\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e377419",
   "metadata": {},
   "source": [
    "### Split into Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cab5ddd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'world!!!',\n",
       " 'this',\n",
       " 'is',\n",
       " 'great',\n",
       " 'for',\n",
       " 'learning',\n",
       " 'nlp',\n",
       " 'preprocessing...',\n",
       " \"i'm\",\n",
       " 'really',\n",
       " 'helpfull!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = lowered.split()\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299952e0",
   "metadata": {},
   "source": [
    "### Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f0c550d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bab9453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world this is great for learning nlp preprocessing im really helpfull'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_punt_list = []\n",
    "for character in lowered:\n",
    "    if character not in string.punctuation:\n",
    "        no_punt_list.append(character)\n",
    "\n",
    "no_punt = \"\".join(no_punt_list)\n",
    "no_punt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20115b6d",
   "metadata": {},
   "source": [
    "## 2. Regular Expressions (Regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05433eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc3c9c7",
   "metadata": {},
   "source": [
    "### Remove Multiple Spaces\n",
    "Replaces multiple consecutive spaces with single space. Text from web scraping often has inconsistent spacing that breaks tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6576a25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world test'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messy_spacing = \"hello    world   test\"\n",
    "fixed_spacing = re.sub(r'\\s+', ' ', messy_spacing)\n",
    "fixed_spacing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c048cde0",
   "metadata": {},
   "source": [
    "### Remove Numbers\n",
    "Removes all digits from text. Numbers often add noise unless they're part of meaningful entities like \"COVID-19\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97b3016a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have  apples and  oranges'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_with_numbers = \"I have 5 apples and 10 oranges\"\n",
    "no_numbers = re.sub(r'\\d+', '', text_with_numbers)\n",
    "no_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944a2024",
   "metadata": {},
   "source": [
    "### Remove URLs and Emails\n",
    "Removes web addresses and email addresses since these are usually not relevant for text analysis and create unique tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22703563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Visit  or email me at '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_text = \"Visit https://example.com or email me at test@email.com\"\n",
    "no_urls = re.sub(r'http\\S+|www\\S+', '', web_text)\n",
    "no_emails = re.sub(r'\\S+@\\S+', '', no_urls)\n",
    "no_emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e727aa",
   "metadata": {},
   "source": [
    "### Remove Special Characters\n",
    "Keeps only alphabetic characters and spaces. Emojis, symbols, and special characters don't contribute to semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67afdf0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello  This has hashtags and mentions'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_text = \"Hello! ðŸš€ This has #hashtags and @mentions\"\n",
    "only_letters = re.sub(r'[^a-zA-Z\\s]', '', special_text)\n",
    "only_letters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4eac8a",
   "metadata": {},
   "source": [
    "## 3. NLTK Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0851f98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/sudarshan/Desktop/Workshop/Code/.venv/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /home/sudarshan/Desktop/Workshop/Code/.venv/lib/python3.10/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/sudarshan/Desktop/Workshop/Code/.venv/lib/python3.10/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/sudarshan/Desktop/Workshop/Code/.venv/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/sudarshan/Desktop/Workshop/Code/.venv/lib/python3.10/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88a872f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sudarshan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sudarshan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/sudarshan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70510e0c",
   "metadata": {},
   "source": [
    "### Better Tokenization\n",
    "NLTK intelligently splits text into tokens, handling contractions and punctuation better than simple split(). For example, \"don't\" becomes [\"do\", \"n't\"] instead of staying as one token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27ca57e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic split: [\"Don't\", 'split', 'contractions', 'badly!', \"It's\", 'important.']\n",
      "NLTK tokens: ['do', \"n't\", 'split', 'contractions', 'badly', '!', 'it', \"'s\", 'important', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "complex_text = \"Don't split contractions badly! It's important.\"\n",
    "basic_split = complex_text.split()\n",
    "nltk_tokens = word_tokenize(complex_text.lower())\n",
    "\n",
    "print(f\"Basic split: {basic_split}\")\n",
    "print(f\"NLTK tokens: {nltk_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf396f51",
   "metadata": {},
   "source": [
    "### Remove Stop Words\n",
    "Removes common function words like \"the\", \"and\", \"is\" that appear frequently but carry little meaning. These words don't help distinguish between different documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ead4a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['hello', 'world', '!', '!', '!', 'this', 'is', 'great', 'for', 'learning', 'nlp', 'preprocessing', '...', 'i', \"'m\", 'really', 'helpfull', '!']\n",
      "Without stop words: ['hello', 'world', '!', '!', '!', 'great', 'learning', 'nlp', 'preprocessing', '...', \"'m\", 'really', 'helpfull', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = word_tokenize(text.lower())\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "print(f\"Original tokens: {tokens}\")\n",
    "print(f\"Without stop words: {filtered_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090362c0",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Stemming reduces words to their root form by removing suffixes. It's a crude but fast way to group related words together. The Porter Stemmer uses a set of rules to chop off word endings. Sometimes it creates non-words like \"studi\" from \"studies\" or \"happi\" from \"happiness\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4aead34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming examples:\n",
      "running â†’ run\n",
      "runs â†’ run\n",
      "easily â†’ easili\n",
      "studies â†’ studi\n",
      "happiness â†’ happi\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words_to_stem = ['running', 'runs', 'easily', 'studies', 'happiness']\n",
    "\n",
    "print(\"Stemming examples:\")\n",
    "for word in words_to_stem:\n",
    "    print(f\"{word} â†’ {stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d77b75",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Lemmatization is more sophisticated than stemming. It reduces words to their dictionary base form (lemma) using vocabulary and morphological analysis. It always produces valid words and understands that \"better\" is the comparative form of \"good\", while \"running\" becomes \"run\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "141256a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/sudarshan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization examples:\n",
      "running â†’ running\n",
      "runs â†’ run\n",
      "easily â†’ easily\n",
      "studies â†’ study\n",
      "happiness â†’ happiness\n",
      "\n",
      "Stemming vs Lemmatization:\n",
      "better â†’ Stem: better, Lemma: better\n",
      "running â†’ Stem: run, Lemma: running\n",
      "studies â†’ Stem: studi, Lemma: study\n",
      "geese â†’ Stem: gees, Lemma: goose\n",
      "feet â†’ Stem: feet, Lemma: foot\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"Lemmatization examples:\")\n",
    "for word in words_to_stem:\n",
    "    print(f\"{word} â†’ {lemmatizer.lemmatize(word)}\")\n",
    "\n",
    "# Compare stemming vs lemmatization\n",
    "print(\"\\nStemming vs Lemmatization:\")\n",
    "comparison_words = ['better', 'running', 'studies', 'geese', 'feet']\n",
    "for word in comparison_words:\n",
    "    stem = stemmer.stem(word)\n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    print(f\"{word} â†’ Stem: {stem}, Lemma: {lemma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3278c380",
   "metadata": {},
   "source": [
    "## 4. Spelling Correction\n",
    "\n",
    "TextBlob uses statistical models to detect and correct spelling mistakes. It's useful for social media text or OCR output, but be careful as it can sometimes change correct words unintentionally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea5f5b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /home/sudarshan/Desktop/Workshop/Code/.venv/lib/python3.10/site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in /home/sudarshan/Desktop/Workshop/Code/.venv/lib/python3.10/site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in /home/sudarshan/Desktop/Workshop/Code/.venv/lib/python3.10/site-packages (from nltk>=3.9->textblob) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/sudarshan/Desktop/Workshop/Code/.venv/lib/python3.10/site-packages (from nltk>=3.9->textblob) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/sudarshan/Desktop/Workshop/Code/.venv/lib/python3.10/site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/sudarshan/Desktop/Workshop/Code/.venv/lib/python3.10/site-packages (from nltk>=3.9->textblob) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdba0ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: I'm really helpfull with lerning\n",
      "Corrected: I'm really helpful with leaning\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "misspelled = \"I'm really helpfull with lerning\"\n",
    "corrected = str(TextBlob(misspelled).correct())\n",
    "print(f\"Original: {misspelled}\")\n",
    "print(f\"Corrected: {corrected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485d03c6",
   "metadata": {},
   "source": [
    "## 5. Handle Contractions\n",
    "\n",
    "Expanding contractions normalizes different forms of the same words. \"Don't\" and \"do not\" should be treated the same way. This is especially important for sentiment analysis and other NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c18faf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: I don't think you're ready, but we'll see\n",
      "Expanded: I do not think you are ready, but we will see\n"
     ]
    }
   ],
   "source": [
    "contractions = {\n",
    "    \"don't\": \"do not\",\n",
    "    \"won't\": \"will not\", \n",
    "    \"can't\": \"cannot\",\n",
    "    \"n't\": \" not\",\n",
    "    \"'re\": \" are\",\n",
    "    \"'ve\": \" have\",\n",
    "    \"'ll\": \" will\",\n",
    "    \"'m\": \" am\"\n",
    "}\n",
    "\n",
    "def expand_contractions(text):\n",
    "    for contraction, expansion in contractions.items():\n",
    "        text = text.replace(contraction, expansion)\n",
    "    return text\n",
    "\n",
    "contract_text = \"I don't think you're ready, but we'll see\"\n",
    "expanded = expand_contractions(contract_text)\n",
    "print(f\"Original: {contract_text}\")\n",
    "print(f\"Expanded: {expanded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de37de1f",
   "metadata": {},
   "source": [
    "## 6. Direct Word Replacement\n",
    "\n",
    "Sometimes you need to standardize domain-specific terms, expand abbreviations, or replace slang with formal equivalents. This is especially useful for social media text or technical documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1e70ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: omg ur AI model is lol good\n",
      "Replaced: oh my god your artificial intelligence model is laugh out loud good\n"
     ]
    }
   ],
   "source": [
    "replacements = {\n",
    "    'u': 'you',\n",
    "    'ur': 'your', \n",
    "    'omg': 'oh my god',\n",
    "    'lol': 'laugh out loud',\n",
    "    'AI': 'artificial intelligence',\n",
    "    'ML': 'machine learning'\n",
    "}\n",
    "\n",
    "def replace_words(text):\n",
    "    words = text.split()\n",
    "    replaced_words = [replacements.get(word, word) for word in words]\n",
    "    return ' '.join(replaced_words)\n",
    "\n",
    "slang_text = \"omg ur AI model is lol good\"\n",
    "replaced = replace_words(slang_text)\n",
    "print(f\"Original: {slang_text}\")\n",
    "print(f\"Replaced: {replaced}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067ec4c4",
   "metadata": {},
   "source": [
    "## Complete Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "138a8c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sudarshan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sudarshan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/sudarshan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# Download required data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Setup preprocessing tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Complete text preprocessing pipeline\"\"\"\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97020bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good text visit wwwtestdoccom info mail us agmailcom 981111111'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original = \"      This is very &&!Good Text. Visit www.testdoc.com for more info or mail us at a@gmail.com! or 981111111 \"\n",
    "cleaned = clean_text(original)\n",
    "\n",
    "cleaned"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
